{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset location & load options\n",
    "dataroot = 'data-64'\n",
    "classes  = 'frogs'\n",
    "workers  = 8\n",
    "\n",
    "# Generator model location\n",
    "netG_path = None\n",
    "netD_path = None\n",
    "from_state = False\n",
    "\n",
    "# Output folder for snapshots\n",
    "outf = 'result-1'\n",
    "\n",
    "# Snapshot frequency (every $snap batches)\n",
    "model_snap = 50\n",
    "image_snap = 50\n",
    "\n",
    "# Snapshot frequency (every $snap_epoch epochs)\n",
    "model_snap_epoch = 1\n",
    "image_snap_epoch = 1\n",
    "\n",
    "# Specify what to snap:\n",
    "snap_state_dict = False\n",
    "shap_model = True\n",
    "\n",
    "# Cuda options\n",
    "cuda = False\n",
    "ngpu = 0\n",
    "\n",
    "# Size of input image (64 or 128)\n",
    "imageSize = 64\n",
    "\n",
    "# Number of channels\n",
    "nc = 3\n",
    "\n",
    "# Batch options\n",
    "batchSize = 128\n",
    "\n",
    "# Latent vector size\n",
    "nz = 10\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "\n",
    "# Number of epochs\n",
    "niter = 50\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.0001\n",
    "\n",
    "# ADAM: beta1\n",
    "beta1 = 0.5\n",
    "\n",
    "# Noise value\n",
    "noiseStd = 0.0\n",
    "noiseStdFinal = 0.0\n",
    "\n",
    "# Real labels range\n",
    "real_label_min = 1.0\n",
    "real_label_max = 1.0\n",
    "\n",
    "# Fake labels range\n",
    "fake_label_min = 0.0\n",
    "fake_label_max = 0.0\n",
    "\n",
    "# Percent of dropout for fake samples generator\n",
    "dropout_probability = 0.0\n",
    "\n",
    "# Generators seed\n",
    "seed = 3951\n",
    "\n",
    "# Prepare for options\n",
    "try:\n",
    "\tos.makedirs(outf)\n",
    "\tos.makedirs(f'{outf}/models')\n",
    "\tos.makedirs(f'{outf}/states')\n",
    "\tos.makedirs(f'{outf}/images')\n",
    "except OSError:\n",
    "\tpass\n",
    "\n",
    "# Random for torch & others\n",
    "if seed is None:\n",
    "\tseed = random.randint(1, 10000)\n",
    "\n",
    "print(\"Random Seed: \", seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# CUDA device select\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "\t\t\t\t\t\ttransform=transforms.Compose([\n",
    "\t\t\t\t\t\t\ttransforms.Resize(imageSize),\n",
    "\t\t\t\t\t\t\ttransforms.CenterCrop(imageSize),\n",
    "\t\t\t\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\t\t\t\ttransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\t\t\t\t\t\t]))\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & initialize from previous state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "\tclassname = m.__class__.__name__\n",
    "\tif classname.find('Conv') != -1:\n",
    "\t\ttorch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "\telif classname.find('BatchNorm') != -1:\n",
    "\t\ttorch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "\t\ttorch.nn.init.zeros_(m.bias)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\tdef __init__(self, ngpu):\n",
    "\t\tsuper(Generator, self).__init__()\n",
    "\t\tself.ngpu = ngpu\n",
    "\t\tif imageSize == 64:\n",
    "\t\t\tself.main = nn.Sequential(\n",
    "\t\t\t\t# input is Z, going into a convolution\n",
    "\t\t\t\tnn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ngf * 8),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Dropout(dropout_probability),\n",
    "\t\t\t\t# state size. (ngf*8) x 4 x 4\n",
    "\t\t\t\tnn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ngf * 4),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Dropout(dropout_probability),\n",
    "\t\t\t\t# state size. (ngf*4) x 8 x 8\n",
    "\t\t\t\tnn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ngf * 2),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Dropout(dropout_probability),\n",
    "\t\t\t\t# state size. (ngf*2) x 16 x 16\n",
    "\t\t\t\tnn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ngf),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Dropout(dropout_probability),\n",
    "\t\t\t\t# state size. (ngf) x 32 x 32\n",
    "\t\t\t\tnn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\tnn.Dropout(dropout_probability)\n",
    "\t\t\t\t# state size. (nc) x 64 x 64\n",
    "\t\t\t)\n",
    "\t\telif imageSize == 128:\n",
    "\t\t\tself.main = nn.Sequential(\n",
    "\t\t\t\t# input is Z, going into a convolution\n",
    "\t\t\t\tnn.ConvTranspose2d(     nz, ngf * 16, 4, 1, 0, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ngf * 16),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Dropout(dropout_probability),\n",
    "\t\t\t\t# state size. (ngf*16) x 4 x 4\n",
    "\t\t\t\tnn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ngf * 8),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Dropout(dropout_probability),\n",
    "\t\t\t\t# state size. (ngf*8) x 8 x 8\n",
    "\t\t\t\tnn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ngf * 4),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Dropout(dropout_probability),\n",
    "\t\t\t\t# state size. (ngf*4) x 16 x 16 \n",
    "\t\t\t\tnn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ngf * 2),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Dropout(dropout_probability),\n",
    "\t\t\t\t# state size. (ngf*2) x 32 x 32\n",
    "\t\t\t\tnn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ngf),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Dropout(dropout_probability),\n",
    "\t\t\t\t# state size. (ngf) x 64 x 64\n",
    "\t\t\t\tnn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\tnn.Dropout(dropout_probability)\n",
    "\t\t\t\t# state size. (nc) x 128 x 128\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tif input.is_cuda and self.ngpu > 1:\n",
    "\t\t\toutput = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "\t\telse:\n",
    "\t\t\toutput = self.main(input)\n",
    "\t\treturn output\n",
    "\t\n",
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self, ngpu):\n",
    "\t\tsuper(Discriminator, self).__init__()\n",
    "\t\tself.ngpu = ngpu\n",
    "\t\tif imageSize == 64:\n",
    "\t\t\tself.main = nn.Sequential(\n",
    "\t\t\t\t# input is (nc) x 64 x 64\n",
    "\t\t\t\tnn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t\t# state size. (ndf) x 32 x 32\n",
    "\t\t\t\tnn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ndf * 2),\n",
    "\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t\t# state size. (ndf*2) x 16 x 16\n",
    "\t\t\t\tnn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ndf * 4),\n",
    "\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t\t# state size. (ndf*4) x 8 x 8\n",
    "\t\t\t\tnn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ndf * 8),\n",
    "\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t\t# state size. (ndf*8) x 4 x 4\n",
    "\t\t\t\tnn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "\t\t\t\tnn.Sigmoid()\n",
    "\t\t\t\t# state size. 1\n",
    "\t\t\t)\n",
    "\t\telif imageSize == 128:\n",
    "\t\t\tself.main = nn.Sequential(\n",
    "\t\t\t\t# input is (nc) x 128 x 128\n",
    "\t\t\t\tnn.Conv2d(nc, ndf, 4, stride=2, padding=1, bias=False), \n",
    "\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t\t# state size. (ndf) x 64 x 64\n",
    "\t\t\t\tnn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ndf * 2),\n",
    "\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t\t# state size. (ndf*2) x 32 x 32\n",
    "\t\t\t\tnn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ndf * 4),\n",
    "\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t\t# state size. (ndf*4) x 16 x 16 \n",
    "\t\t\t\tnn.Conv2d(ndf * 4, ndf * 8, 4, stride=2, padding=1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ndf * 8),\n",
    "\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t\t# state size. (ndf*8) x 8 x 8\n",
    "\t\t\t\tnn.Conv2d(ndf * 8, ndf * 16, 4, stride=2, padding=1, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(ndf * 16),\n",
    "\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t\t# state size. (ndf*16) x 4 x 4\n",
    "\t\t\t\tnn.Conv2d(ndf * 16, 1, 4, stride=1, padding=0, bias=False),\n",
    "\t\t\t\tnn.Sigmoid()\n",
    "\t\t\t\t# state size. 1\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tif input.is_cuda and self.ngpu > 1:\n",
    "\t\t\toutput = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "\t\telse:\n",
    "\t\t\toutput = self.main(input)\n",
    "\n",
    "\t\treturn output.view(-1, 1).squeeze(1)\n",
    "\n",
    "if from_state:\n",
    "\tnetG = Generator(ngpu)\n",
    "\tnetG.apply(weights_init)\n",
    "\tif netG_path is not None:\n",
    "\t\tnetG.load_state_dict(torch.load(netG_path))\n",
    "\t#print(netG)\n",
    "\n",
    "\tnetD = Discriminator(ngpu)\n",
    "\tnetD.apply(weights_init)\n",
    "\tif netD_path is not None:\n",
    "\t\tnetD.load_state_dict(torch.load(netD_path))\n",
    "\t#print(netD)\n",
    "else:\n",
    "\tif netG_path is not None:\n",
    "\t\tnetG = torch.load(netG_path)\n",
    "\telse:\n",
    "\t\tnetG = Generator(ngpu)\n",
    "\t\tnetG.apply(weights_init)\n",
    "\t#print(netG)\n",
    "\t\n",
    "\tif netD_path is not None:\n",
    "\t\tnetD = torch.load(netD_path)\n",
    "\telse:\n",
    "\t\tnetD = Discriminator(ngpu)\n",
    "\t\tnetD.apply(weights_init)\n",
    "\t#print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_D(netD, netG, criterion, optimizerD, niter, nbatch, nz, lr, beta1, real_label_min, real_label_max, fake_label_min, fake_label_max, noiseStd, noiseStdFinal, dataloader, device='cpu'):\n",
    "\n",
    "\tif optimizerD == None:\n",
    "\t\toptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\t\n",
    "\tnetG.train()\n",
    "\tnetD.train()\n",
    "\n",
    "\tfor epoch in range(niter):\n",
    "\t\t# Decay noise depending on iterations count\n",
    "\t\tnoiseStdCurrent = noiseStd + (noiseStdFinal - noiseStd) / (niter - epoch)\n",
    "\n",
    "\t\tfor i, data in enumerate(dataloader, 0):\n",
    "\t\t\tif nbatch is not None and i >= nbatch:\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\t# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "\n",
    "\t\t\tbatch_size = data[0].size(0)\n",
    "\t\t\tlabel_dtype = data[0].dtype\n",
    "\n",
    "\t\t\treal_noise = torch.randn(data[0].size()) * noiseStdCurrent\n",
    "\t\t\treal_label = torch.rand((batch_size,), dtype=label_dtype, device=device) * (real_label_max - real_label_min) + real_label_min\n",
    "\t\t\tfake_label = torch.rand((batch_size,), dtype=label_dtype, device=device) * (fake_label_max - fake_label_min) + fake_label_min\n",
    "\n",
    "\t\t\t# Train with real\n",
    "\t\t\tnetD.zero_grad()\n",
    "\t\t\treal_sample = data[0] + real_noise\n",
    "\t\t\t\n",
    "\t\t\t# Backpropagate output for real sample\n",
    "\t\t\toutput = netD(real_sample)\n",
    "\t\t\terrD_real = criterion(output, real_label)\n",
    "\t\t\terrD_real.backward()\n",
    "\t\t\terrD_real_mean = output.mean()\n",
    "\n",
    "\t\t\t# Train with fake\n",
    "\t\t\tnoise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\t\t\tfake_sample = netG(noise)\n",
    "\t\t\t\n",
    "\t\t\toutput = netD(fake_sample) # .detach())\n",
    "\t\t\terrD_fake = criterion(output, fake_label)\n",
    "\t\t\terrD_fake.backward()\n",
    "\t\t\terrD_fake_mean = output.mean()\n",
    "\t\t\t\n",
    "\t\t\terrD = errD_real_mean + errD_fake_mean\n",
    "\t\t\toptimizerD.step()\n",
    "\n",
    "\t\t\tprint(f'[{epoch}/{niter}][{i}/{len(dataloader)}] Loss_D: {errD.item():.4f} D(x): {errD_real_mean.item():.4f}')\n",
    "\t\t\t\n",
    "\tnetG.eval()\n",
    "\tnetD.eval()\n",
    "\n",
    "def train_G(netD, netG, criterion, optimizerG, niter, nbatch, nz, lr, beta1, real_label_min, real_label_max, dataloader, device='cpu'):\n",
    "\n",
    "\tif optimizerG == None:\n",
    "\t\toptimizerG = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\t\n",
    "\tnetG.train()\n",
    "\tnetD.train()\n",
    "\n",
    "\tfor epoch in range(niter):\n",
    "\t\tfor i, data in enumerate(dataloader, 0):\n",
    "\t\t\tif nbatch is not None and i >= nbatch:\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\t# (2) Update G network: maximize log(D(G(z)))\n",
    "\n",
    "\t\t\tbatch_size = data[0].size(0)\n",
    "\t\t\tlabel_dtype = data[0].dtype\n",
    "\n",
    "\t\t\treal_label = torch.rand((batch_size,), dtype=label_dtype, device=device) * (real_label_max - real_label_min) + real_label_min\n",
    "\n",
    "\t\t\t# Train with fake\n",
    "\t\t\tnoise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\t\t\tfake_sample = netG(noise)\n",
    "\t\t\t\n",
    "\t\t\t# Assign true label\n",
    "\t\t\tnetG.zero_grad()\n",
    "\t\t\toutput = netD(fake_sample)\n",
    "\t\t\terrG = criterion(output, real_label)\n",
    "\t\t\terrG.backward()\n",
    "\t\t\terrG_mean = output.mean()\n",
    "\n",
    "\t\t\toptimizerG.step()\n",
    "\n",
    "\t\t\tprint(f'[{epoch}/{niter}][{i}/{len(dataloader)}] Loss_G: {errG.item():.4f} D(G(z)): {errG_mean.item():.4f}')\n",
    "\t\t\t\n",
    "\tnetG.eval()\n",
    "\tnetD.eval()\n",
    "\n",
    "def train_DG(netD, netG, criterion, optimizerD, optimizerG, niter, nbatch, nz, lr, beta1, real_label_min, real_label_max, fake_label_min, fake_label_max, noiseStd, noiseStdFinal, dataloader, device='cpu'):\n",
    "\n",
    "\tif optimizerD == None:\n",
    "\t\toptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\t\t\n",
    "\tif optimizerG == None:\n",
    "\t\toptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\t\n",
    "\tnetG.train()\n",
    "\tnetD.train()\n",
    "\n",
    "\tfor epoch in range(niter):\n",
    "\t\t# Decay noise depending on iterations count\n",
    "\t\tnoiseStdCurrent = noiseStd + (noiseStdFinal - noiseStd) / (niter - epoch)\n",
    "\n",
    "\t\tfor i, data in enumerate(dataloader, 0):\n",
    "\t\t\tif nbatch is not None and i >= nbatch:\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\t# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "\n",
    "\t\t\tbatch_size = data[0].size(0)\n",
    "\t\t\tlabel_dtype = data[0].dtype\n",
    "\n",
    "\t\t\treal_noise = torch.randn(data[0].size()) * noiseStdCurrent\n",
    "\t\t\treal_label = (torch.rand((batch_size,), dtype=label_dtype, device=device) * (real_label_max - real_label_min) + real_label_min)\n",
    "\t\t\tfake_label = (torch.rand((batch_size,), dtype=label_dtype, device=device) * (fake_label_max - fake_label_min) + fake_label_min)\n",
    "\n",
    "\t\t\t# Train with real\n",
    "\t\t\tnetD.zero_grad()\n",
    "\t\t\treal_sample = (data[0] + real_noise)\n",
    "\t\t\t\n",
    "\t\t\t# Backpropagate output for real sample\n",
    "\t\t\toutput = netD(real_sample)\n",
    "\t\t\terrD_real = criterion(output, real_label)\n",
    "\t\t\t# errD_real.backward()\n",
    "\t\t\terrD_real_mean = output.mean()\n",
    "\n",
    "\t\t\t# Train with fake\n",
    "\t\t\tnoise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\t\t\tfake_sample = netG(noise)\n",
    "\t\t\t\n",
    "\t\t\toutput = netD(fake_sample.detach())\n",
    "\t\t\terrD_fake = criterion(output, fake_label)\n",
    "\t\t\terrD_fake.backward()\n",
    "\t\t\terrD_fake_mean = output.mean()\n",
    "\t\t\t\n",
    "\t\t\terrD = errD_real_mean + errD_fake_mean\n",
    "\t\t\toptimizerD.step()\n",
    "\t\t\t\n",
    "\t\t\t# Assign true label\n",
    "\t\t\tnetG.zero_grad()\n",
    "\t\t\toutput = netD(fake_sample)\n",
    "\t\t\terrG = criterion(output, real_label)\n",
    "\t\t\terrG.backward()\n",
    "\t\t\terrG_mean = output.mean()\n",
    "\n",
    "\t\t\toptimizerG.step()\n",
    "\n",
    "\t\t\tprint(f'[{epoch}/{niter}][{i}/{len(dataloader)}] Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} D(x): {errD_real_mean.item():.4f} D(G(z)): {errG_mean.item():.4f}')\n",
    "\t\t\t\n",
    "\tnetG.eval()\n",
    "\tnetD.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DG(netD, netG, criterion, optimizerD, optimizerG, niter, None, nz, lr, beta1, real_label_min, real_label_max, fake_label_min, fake_label_max, noiseStd, noiseStdFinal, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using modified original code from Pytorch article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "fixed_noise = torch.randn(batchSize, nz, 1, 1, device=device)\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) # SGD(netD.parameters(), lr=lr)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "netG.train()\n",
    "netD.train()\n",
    "\n",
    "for epoch in range(niter):\n",
    "\t# Decay noise depending on iterations count\n",
    "\tnoiseStdCurrent = noiseStd + (noiseStdFinal - noiseStd) / (niter - epoch)\n",
    "\n",
    "\tfor i, data in enumerate(dataloader, 0):\n",
    "\t\t############################\n",
    "\t\t# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "\t\t###########################\n",
    "\n",
    "\t\t# train with real\n",
    "\t\tnetD.zero_grad()\n",
    "\t\treal_cpu = data[0]\n",
    "\n",
    "\t\t# Add noise\n",
    "\t\treal_cpu = real_cpu + torch.randn(real_cpu.size()) * noiseStdCurrent\n",
    "\n",
    "\t\tbatch_size = real_cpu.size(0)\n",
    "\n",
    "\t\t# Generate label distribution\n",
    "\t\treal_label = torch.rand((batch_size,), dtype=real_cpu.dtype, device=device) * (real_label_max - real_label_min) + real_label_min # torch.full((batch_size,), real_label, dtype=real_cpu.dtype, device=device)\n",
    "\n",
    "\t\t# Flip labels with chance of 0.001\n",
    "\t\t# ?\n",
    "\n",
    "\t\toutput = netD(real_cpu)\n",
    "\t\terrD_real = criterion(output, real_label) ###############################################################\n",
    "\t\terrD_real.backward()\n",
    "\t\tD_x = output.mean().item()\n",
    "\n",
    "\t\t# train with fake\n",
    "\t\tnoise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\t\tfake = netG(noise)\n",
    "\n",
    "\t\t# Generate label distribution\n",
    "\t\tfake_label = torch.rand((batch_size,), dtype=real_cpu.dtype, device=device) * (fake_label_max - fake_label_min) + fake_label_min # .fill_(fake_label)\n",
    "\n",
    "\t\toutput = netD(fake.detach())\n",
    "\t\terrD_fake = criterion(output, fake_label)\n",
    "\t\terrD_fake.backward()\n",
    "\t\tD_G_z1 = output.mean().item()\n",
    "\t\terrD = errD_real + errD_fake\n",
    "\t\toptimizerD.step()\n",
    "\n",
    "\t\t############################\n",
    "\t\t# (2) Update G network: maximize log(D(G(z)))\n",
    "\t\t###########################\n",
    "\t\tnetG.zero_grad()\n",
    "\t\toutput = netD(fake)\n",
    "\t\terrG = criterion(output, real_label)\n",
    "\t\terrG.backward()\n",
    "\t\tD_G_z2 = output.mean().item()\n",
    "\t\toptimizerG.step()\n",
    "\n",
    "\t\tprint('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' % (epoch, niter, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\t\t\n",
    "\t\t# Snap batches\n",
    "\t\tif i % model_snap == 0:\n",
    "\t\t\tif shap_model:\n",
    "\t\t\t\ttorch.save(netG, f'{outf}/models/netG_res_{imageSize}_seed_{seed}_epoch_{epoch}_iter_{i}.pth')\n",
    "\t\t\t\ttorch.save(netD, f'{outf}/models/netD_res_{imageSize}_seed_{seed}_epoch_{epoch}_iter_{i}.pth')\n",
    "\t\t\tif snap_state_dict:\n",
    "\t\t\t\ttorch.save(netG.state_dict(), f'{outf}/states/netG_res_{imageSize}_seed_{seed}_epoch_{epoch}_iter_{i}.pth')\n",
    "\t\t\t\ttorch.save(netD.state_dict(), f'{outf}/states/netD_res_{imageSize}_seed_{seed}_epoch_{epoch}_iter_{i}.pth')\n",
    "\t\t\n",
    "\t\tif i % image_snap == 0:\n",
    "\t\t\tnetG.eval()\n",
    "\t\t\tnetD.eval()\n",
    "\t\t\tvutils.save_image(real_cpu, f'{outf}/images/res_{imageSize}_seed_{seed}.png', normalize=True)\n",
    "\t\t\tfake = netG(fixed_noise)\n",
    "\t\t\tvutils.save_image(fake.detach(), f'{outf}/images/res_{imageSize}_seed_{seed}_epoch_{epoch}_iter_{i}.png', normalize=True)\n",
    "\t\t\tnetG.train()\n",
    "\t\t\tnetD.train()\n",
    "\t\n",
    "\t# Snap epochs\n",
    "\tif epoch % model_snap_epoch == 0:\n",
    "\t\tif shap_model:\n",
    "\t\t\ttorch.save(netG, f'{outf}/models/netG_res_{imageSize}_seed_{seed}_epoch_{epoch}_final.pth')\n",
    "\t\t\ttorch.save(netD, f'{outf}/models/netD_res_{imageSize}_seed_{seed}_epoch_{epoch}_final.pth')\n",
    "\t\tif snap_state_dict:\n",
    "\t\t\ttorch.save(netG.state_dict(), f'{outf}/states/netG_res_{imageSize}_seed_{seed}_epoch_{epoch}_final.pth')\n",
    "\t\t\ttorch.save(netD.state_dict(), f'{outf}/states/netD_res_{imageSize}_seed_{seed}_epoch_{epoch}_final.pth')\n",
    "\t\n",
    "\tif epoch % image_snap_epoch == 0:\n",
    "\t\tnetG.eval()\n",
    "\t\tnetD.eval()\n",
    "\t\tvutils.save_image(real_cpu, f'{outf}/images/res_{imageSize}_seed_{seed}.png', normalize=True)\n",
    "\t\tfake = netG(fixed_noise)\n",
    "\t\tvutils.save_image(fake.detach(), f'{outf}/images/res_{imageSize}_seed_{seed}_epoch_{epoch}_final.png', normalize=True)\n",
    "\t\tnetG.train()\n",
    "\t\tnetD.train()\n",
    "\t\n",
    "# Snap last\n",
    "if shap_model:\n",
    "\ttorch.save(netG, f'{outf}/models/netG_res_{imageSize}_seed_{seed}_final.pth')\n",
    "\ttorch.save(netD, f'{outf}/models/netD_res_{imageSize}_seed_{seed}_final.pth')\n",
    "if snap_state_dict:\n",
    "\ttorch.save(netG.state_dict(), f'{outf}/states/netG_res_{imageSize}_seed_{seed}_final.pth')\n",
    "\ttorch.save(netD.state_dict(), f'{outf}/states/netD_res_{imageSize}_seed_{seed}_final.pth')\n",
    "\n",
    "netG.eval()\n",
    "netD.eval()\n",
    "vutils.save_image(real_cpu, f'{outf}/images/res_{imageSize}_seed_{seed}.png', normalize=True)\n",
    "fake = netG(fixed_noise)\n",
    "vutils.save_image(fake.detach(), f'{outf}/images/res_{imageSize}_seed_{seed}_final.png', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate frogs by max score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_best_image(samples, best_limit_min, best_limit_max, max_fails, nz, device='cpu', resize_to=(64, 64), resize_sampling=Image.BICUBIC):\n",
    "\twith torch.no_grad():\n",
    "\t\tfor m in range(max_fails):\n",
    "\n",
    "\t\t\t# Generate N images\n",
    "\t\t\tnoise = torch.randn(samples, nz, 1, 1, device=device)\n",
    "\t\t\tfakes = netG(noise)\n",
    "\n",
    "\t\t\t# Select best for D metric\n",
    "\t\t\toutput_fakes = netD(fakes)\n",
    "\n",
    "\t\t\t# Select best < best_limit\n",
    "\t\t\tmasked_output_fakes = torch.masked_select(output_fakes, output_fakes.ge(best_limit_min))\n",
    "\t\t\tmasked_output_fakes = torch.masked_select(masked_output_fakes, masked_output_fakes.le(best_limit_max))\n",
    "\t\t\t\n",
    "\t\t\tif len(masked_output_fakes) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t# Finx max with limit\n",
    "\t\t\tbest_index = torch.argmax(masked_output_fakes)\n",
    "\t\t\tbest = fakes[best_index]\n",
    "\t\t\tbest_rate = masked_output_fakes[best_index]\n",
    "\n",
    "\t\t\tbest_image = transforms.ToPILImage()(best * 0.5 + 0.5).convert('RGB').resize(resize_to, resample=resize_sampling)\n",
    "\t\t\t\n",
    "\t\t\treturn best, best_image, best_rate\n",
    "\t\t\n",
    "\treturn None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "\n",
    "netD = torch.load('netD.pth')\n",
    "netG = torch.load('netG.pth')\n",
    "\n",
    "netD.eval()\n",
    "netG.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES        = 32\n",
    "MAX_FAILS      = 25\n",
    "BEST_LIMIT_MIN = 0.0\n",
    "BEST_LIMIT_MAX = 1.0\n",
    "RESIZE_TO      = (64, 64)\n",
    "RESIZE_MODE    = Image.BICUBIC\n",
    "\n",
    "best, best_image, best_rate = pick_best_image(SAMPLES, BEST_LIMIT_MIN, BEST_LIMIT_MAX, MAX_FAILS, nz, resize_to=RESIZE_TO, resize_sampling=RESIZE_MODE)\n",
    "\n",
    "print(f'Best rate = {best_rate}')\n",
    "display(best_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES     = 1024\n",
    "SAMPLES        = 8\n",
    "BEST_LIMIT_MAX = 1.0\n",
    "BEST_LIMIT_MIN = 0.0\n",
    "MAX_FAILS      = 50\n",
    "RESIZE_TO      = (64, 64)\n",
    "RESIZE_MODE    = Image.BICUBIC\n",
    "IMAGES_DIR     = 'generated-images'\n",
    "\n",
    "ts = round(time.time())\n",
    "\n",
    "try:\n",
    "\tos.mkdir(IMAGES_DIR)\n",
    "except:\n",
    "\tpass\n",
    "try:\n",
    "\tos.mkdir(f'{IMAGES_DIR}/{ts}')\n",
    "except:\n",
    "\tpass\n",
    "\n",
    "for i in range(NUM_IMAGES):\n",
    "\tprint(f'Image {i+1} of {NUM_IMAGES}')\n",
    "\tbest, best_image, best_rate = pick_best_image(SAMPLES, BEST_LIMIT_MIN, BEST_LIMIT_MAX, MAX_FAILS, nz)\n",
    "\tbest_image.resize(RESIZE_TO, resample=RESIZE_MODE).convert('RGB').save(f'{IMAGES_DIR}/{ts}/image_{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES    = 1024\n",
    "GRID_SIZE     = 8\n",
    "OUTPUT_FILDER = 'generated-grid'\n",
    "RESIZE_TO     = (64, 64)\n",
    "RESIZE_MODE   = Image.BICUBIC\n",
    "LINE_SIZE     = 2\n",
    "\n",
    "try:\n",
    "\tos.mkdir(OUTPUT_FILDER)\n",
    "except:\n",
    "\tpass\n",
    "\n",
    "ts = round(time.time())\n",
    "\n",
    "def image_grid(imgs, rows, cols, line_size=2):\n",
    "\tassert len(imgs) == rows*cols\n",
    "\n",
    "\tw, h = imgs[0].size\n",
    "\tgrid = Image.new('RGB', size=(cols * (w + line_size), rows * (h + line_size)))\n",
    "\n",
    "\tfor i, img in enumerate(imgs):\n",
    "\t\tgrid.paste(img, box=(i % cols * (w + line_size) + line_size // 2, i // cols * (h + line_size) + line_size // 2))\n",
    "\t\n",
    "\treturn grid\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor i in range(NUM_IMAGES):\n",
    "\t\tprint(f'Generating {i} / {NUM_IMAGES}')\n",
    "\n",
    "\t\t# Generate N images\n",
    "\t\tnoise = torch.randn(GRID_SIZE ** 2, nz, 1, 1, device=device)\n",
    "\t\tfakes = netG(noise)\n",
    "\n",
    "\t\timage_fakes = [ transforms.ToPILImage()(img * 0.5 + 0.5).convert('RGB').resize(RESIZE_TO, resample=RESIZE_MODE) for img in fakes]\n",
    "\t\timage_grid(image_fakes, GRID_SIZE, GRID_SIZE, LINE_SIZE).save(f'{OUTPUT_FILDER}/image_{i}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
